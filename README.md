<div align="center">

# ğŸ•·ï¸ Arachnida

### _Mon scraper d'images png, jpg, gif et bmp_

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-blue.svg?style=for-the-badge&logo=python&logoColor=white" alt="Python Version">
  <img src="https://img.shields.io/badge/Status-Active-success.svg?style=for-the-badge" alt="Status">
  <img src="https://img.shields.io/badge/Platform-Linux%20%7C%20macOS%20%7C%20Windows-lightgrey.svg?style=for-the-badge" alt="Platform">
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Threads-Multi--threaded-orange.svg?style=flat-square" alt="Threads">
  <img src="https://img.shields.io/badge/Recursion-Configurable-blueviolet.svg?style=flat-square" alt="Recursion">
  <img src="https://img.shields.io/badge/Logging-Full%20Support-informational.svg?style=flat-square" alt="Logging">
</p>

<div align="center">
```
           ____                      ,
          /---.'.__             ____//
               '--.\           /.---'
          _______  \\         //
        /.------.\  \|      .'/  ______
       //  ___  \ \ ||/|\  //  _/_----.\__
      |/  /.-.\  \ \:|< >|// _/.'..\   '--'
         //   \'. | \'.|.'/ /_/ /  \\
        //     \ \_\/" ' ~\-'.-'    \\
       //       '-._| :H: |'-.__     \\
      //           (/'==='\)'-._\     ||
      ||                        \\    \|
      ||                         \\    '
      |/                          \\
```
</div>
</div>

---

## ğŸ“‹ Table des matiÃ¨res

- [âœ¨ FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [ğŸš€ Installation rapide](#-installation-rapide)
- [ğŸ’» Utilisation](#-utilisation)
- [âš™ï¸ Options CLI](#ï¸-options-cli)
- [ğŸ“Š Exemple de sortie](#-exemple-de-sortie)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸ“ Structure du projet](#-structure-du-projet)
- [ğŸ”§ Configuration avancÃ©e](#-configuration-avancÃ©e)
- [ğŸ“ Logging](#-logging)
- [âš ï¸ Avertissements](#ï¸-avertissements)
- [â“ FAQ](#-faq)
- [ğŸ¤ Contribution](#-contribution)
- [ğŸ“„ Licence](#-licence)

---

## âœ¨ FonctionnalitÃ©s

<div align="center">

| FonctionnalitÃ© | Description |
|:---:|:---|
| ğŸ”„ | **Crawling rÃ©cursif** avec profondeur configurable |
| âš¡ | **Multi-threading** pour des tÃ©lÃ©chargements rapides |
| ğŸ¯ | **DÃ©tection intelligente** des duplicatas |
| ğŸ“Š | **Barres de progression** Ã©lÃ©gantes avec `tqdm` |
| ğŸ“ | **Logging complet** (fichier + console) |
| ğŸ¨ | **Interface CLI** intuitive avec ASCII art |
| ğŸ” | **Filtrage automatique** des formats d'image (.jpg, .png, .gif, .bmp) |
| ğŸ’¾ | **Organisation hiÃ©rarchique** des tÃ©lÃ©chargements |

</div>

---

## ğŸš€ Installation rapide

### PrÃ©requis

- **Python 3.8+** recommandÃ©
- `pip` pour installer les dÃ©pendances

### Ã‰tapes

```bash
# Cloner le repository
git clone https://github.com/monsieurCanard/Arachnida.git
cd Arachnida

# Installer les dÃ©pendances
python3 -m pip install -r requirements.txt

# Ou installer manuellement
python3 -m pip install requests tqdm
```

> ğŸ’¡ **Astuce** : Utilisez un environnement virtuel pour isoler les dÃ©pendances
> ```bash
> python3 -m venv .venv
> source .venv/bin/activate  # Linux/macOS
> # .venv\Scripts\activate    # Windows
> ```

---

## ğŸ’» Utilisation

### Commande de base

```bash
dist/Spider <URL> [OPTIONS]
```

### ğŸ“– Exemples pratiques

<details>
<summary>ğŸ”¹ <b>Crawl simple (sans rÃ©cursivitÃ©)</b></summary>

```bash
dist/Spider https://example.com
```
</details>

<details>
<summary>ğŸ”¹ <b>Crawl rÃ©cursif avec profondeur limitÃ©e</b></summary>

```bash
dist/Spider https://example.com -r -l 2
```
</details>

<details>
<summary>ğŸ”¹ <b>Personnaliser le dossier de destination</b></summary>

```bash
dist/Spider https://example.com -p my_images
```
</details>

<details>
<summary>ğŸ”¹ <b>Activer les logs dÃ©taillÃ©s</b></summary>

```bash
dist/Spider https://example.com --log DEBUG
```
</details>

<details>
<summary>ğŸ”¹ <b>Configuration complÃ¨te</b></summary>

```bash
dist/Spider https://example.com \
  -r \
  -l 3 \
  -p downloads/images \
  --log INFO
```
</details>

---

## âš™ï¸ Options CLI

| Option | Raccourci | Type | DÃ©faut | Description |
|:---:|:---:|:---:|:---:|:---|
| `url` | - | `string` | **requis** | ğŸŒ URL cible Ã  scraper |
| `--recurse` | `-r` | `flag` | `False` | ğŸ”„ Activer le crawling rÃ©cursif |
| `--level` | `-l` | `int` | `5` | ğŸ“Š Profondeur de rÃ©cursion (1-10) |
| `--path` | `-p` | `string` | `data` | ğŸ“ Dossier de destination |
| `--log` | - | `string` | `None` | ğŸ“ Niveau de log (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) |

> âš ï¸ **Note** : Si aucun schÃ©ma n'est fourni dans l'URL, `https://` est automatiquement ajoutÃ©.

---

## ğŸ“Š Exemple de sortie

<!-- <details>
<summary>Cliquez pour voir un exemple complet d'exÃ©cution</summary> -->

```bash
2025-11-22 09:19:32,485 [INFO] Arachnida Spider started.
    ========================================
          Arachnida - Web Scraper
    ========================================
           ____                      ,
          /---.'.__             ____//
               '--.\           /.---'
          _______  \\         //
        /.------.\  \|      .'/  ______
       //  ___  \ \ ||/|\  //  _/_----.\__
      |/  /.-.\  \ \:|< >|// _/.'..\   '--'
         //   \'. | \'.|.'/ /_/ /  \\
        //     \ \_\/" ' ~\-'.-'    \\
       //       '-._| :H: |'-.__     \\
      //           (/'==='\)'-._\     ||
      ||                        \\    \|
      ||                         \\    '
      |/                          \\
                                   ||
                                   ||
                                   \\
    ========================================
        TARGET URL: https://example.com
        RECURSION: True
        DEPTH LEVEL: 2
    ========================================
        LET'S GET ALL THE IMAGES!
    ++++++++++++++++++++++++++++++++++++++++

2025-11-22 09:19:32,486 [INFO] Fetching main page: https://example.com
2025-11-22 09:19:32,691 [INFO] Found 6 images and 461 links on the page.
2025-11-22 09:19:32,726 [INFO] Starting crawl with recursion level 2.

# Phase de crawling
Spider charging caffeineâ€¦ crawling fasterâ€¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:42<00:00, 10.9it/s]

                     Total images to download: 123 images.
                     Do you want to download the images? (y/n): y

                     Starting image download...

# Phase de tÃ©lÃ©chargement
Negotiating with TCP like it's a hostage situation.: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [00:22<00:00, 5.45it/s]

    ========================================
          Arachnida - Scraping Complete
    ========================================
    ++ 120 downloaded ( ~ 8.54 MB)
    -----------------------------------
    -- 3 duplicates
    -----------------------------------
    -- 0 errors.
    -----------------------------------
    ===================================
      Spider returning to the shadowsâ€¦
            As all spiders do.
    ===================================
```

<!-- </details> -->

---

## ğŸ—ï¸ Architecture

```mermaid
graph TD
    A[prog.py] -->|CrÃ©e| B[Scraper]
    B -->|Parse args| C[Parser]
    B -->|Configure| D[Logger]
    B -->|Fetch HTML| E[Page Web]
    E -->|Parse| F[Liens & Images]
    F -->|Recursion?| G{Oui/Non}
    G -->|Oui| H[Worker.run]
    G -->|Non| I[Download direct]
    H -->|ThreadPool| J[Crawl rÃ©cursif]
    J --> I
    I -->|Worker.download_images| K[Fichiers locaux]
    K --> L[data/netloc/path/]
```

### ğŸ”„ Flux d'exÃ©cution

1. **Initialisation** : CrÃ©ation de l'objet `Scraper` et parsing des arguments CLI
2. **Configuration** : Setup du logger et du `ThreadPoolExecutor`
3. **Fetch** : RÃ©cupÃ©ration de la page HTML principale
4. **Parsing** : Extraction des liens et images via `HTMLParser`
5. **Crawling** : *(optionnel)* Parcours rÃ©cursif des liens trouvÃ©s
6. **Download** : TÃ©lÃ©chargement multi-threadÃ© des images
7. **Rapport** : Affichage des statistiques finales

---

## ğŸ“ Structure du projet

```
Arachnida/
â”œâ”€â”€ ğŸ“„ README.md                 # Ce fichier
â”œâ”€â”€ ğŸ“„ requirements.txt          # DÃ©pendances Python
â”œâ”€â”€ ğŸ“„ spider.log                # Logs d'exÃ©cution (gÃ©nÃ©rÃ©)
â”œâ”€â”€ ğŸ“‚ Spider/                   # Code source
â”‚   â”œâ”€â”€ ğŸ prog.py              # Point d'entrÃ©e principal
â”‚   â”œâ”€â”€ ğŸ Scraper.py           # Logique du scraper
â”‚   â”œâ”€â”€ ğŸ Parser.py            # Parsing HTML + CLI args
â”‚   â”œâ”€â”€ ğŸ Worker.py            # TÃ¢ches threadÃ©es
â”‚   â”œâ”€â”€ ğŸ Logger.py            # Configuration logging
â”‚   â”œâ”€â”€ ğŸ utils.py             # Fonctions utilitaires
â”‚   â””â”€â”€ ğŸ print.py             # BanniÃ¨res ASCII
â””â”€â”€ ğŸ“‚ data/                     # Images tÃ©lÃ©chargÃ©es (gÃ©nÃ©rÃ©)
    â””â”€â”€ ğŸ“‚ <netloc>/
        â””â”€â”€ ğŸ“‚ <path>/
            â””â”€â”€ ğŸ–¼ï¸ image.jpg
```

---

## ğŸ”§ Configuration avancÃ©e

### ğŸ¨ Personnaliser les messages de progression

Ã‰ditez `prog.py` (lignes 15-22) pour modifier les descriptions des barres `tqdm` :

```python
progress_bar_desc = [
    "Votre message personnalisÃ© 1",
    "Votre message personnalisÃ© 2",
    "Votre message personnalisÃ© 3"
]
```

### ğŸ“ Logging

Le logger Ã©crit simultanÃ©ment dans :
- **Fichier** : `spider.log` (niveau `DEBUG` - tout est enregistrÃ©)
- **Console** : stdout (niveau `INFO` - messages importants uniquement)

#### Configuration dans `Logger.py`

```python
# File handler - enregistre tout
fh = logging.FileHandler("spider.log")
fh.setLevel(logging.DEBUG)

# Stream handler - affiche INFO+
sh = logging.StreamHandler()
sh.setLevel(logging.INFO)
```

#### Utilisation

```bash
# Logs normaux (INFO)
dist/Spider https://example.com --log INFO

# Logs dÃ©taillÃ©s (DEBUG)
dist/Spider https://example.com --log DEBUG

# Logs minimaux (WARNING)
dist/Spider https://example.com --log WARNING
```

---

## âš ï¸ Avertissements

> ğŸš¨ **Important** : Utilisez Arachnida de maniÃ¨re responsable

| âš ï¸ | ConsidÃ©ration |
|:---:|:---|
| ğŸ¤– | Respectez le fichier `robots.txt` des sites web |
| â±ï¸ | Ã‰vitez de surcharger les serveurs avec trop de requÃªtes |
| ğŸ“œ | VÃ©rifiez les conditions d'utilisation des sites cibles |
| ğŸ”’ | Certains sites peuvent bloquer les scrapers |
| âš–ï¸ | Assurez-vous d'avoir le droit de tÃ©lÃ©charger le contenu |

### Limitations connues

- â³ Timeout par dÃ©faut : 5 secondes (peut Ãªtre insuffisant pour certains serveurs)
- ğŸ”„ Gestion d'erreurs basique (pas de retry automatique)
- ğŸ“¸ DÃ©tection de duplicatas basÃ©e uniquement sur le chemin du fichier

---

<div align="center">

### ğŸ’– Merci d'utiliser Arachnida !

<p align="center">
  <img src="https://img.shields.io/badge/Made%20with-Python-1f425f.svg?style=for-the-badge&logo=python" alt="Made with Python">
  <img src="https://img.shields.io/badge/Maintained-Yes-green.svg?style=for-the-badge" alt="Maintained">
</p>

**Si ce projet vous a Ã©tÃ© utile, n'hÃ©sitez pas Ã  lui donner une â­ !**

[â¬† Retour en haut](#-arachnida)

</div>

